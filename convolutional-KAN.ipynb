{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Convolutional Kolmogorov-Arnold Network (CKAN)\n",
        "\n",
        "The CKAN introduction and the experiments are explained in this paper (https://arxiv.org/pdf/2406.13155)\n",
        "\n",
        "####Agenda\n",
        "1. Installation\n",
        "2. What is a KAN?\n",
        "3. What is a KAN Convolution?\n",
        "  1. Idea in the nutshell: KAN Convolutions are very similar to convolutions, but instead of applying the dot product between the kernel and the corresponding pixels in the image, we apply a Learnable Non Linear activation function to each element, and then add them up.\n",
        "  2. Convolution from scratch: https://github.com/detkov/Convolution-From-Scratch/\n",
        "\n",
        "\n",
        "4. Parameters in a KAN Convolution\n",
        "5. Results of convolutional layers with KAN\n",
        "6. Conclusion\n",
        "\n",
        "7. Work in progress\n",
        "\n",
        "    A. Experiments on more complex datasets.\n",
        "\n",
        "    B. Hyperparameter tuning with Random Search.\n",
        "    \n",
        "    C. Experiments with more architectures.\n",
        "    \n",
        "    D. Dinamically updating grid ranges.\n"
      ],
      "metadata": {
        "id": "E967biRW-sBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cloning the CKAN git repository\n",
        "!git clone https://github.com/AntonioTepsich/Convolutional-KANs.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IA9kSwgAG7q",
        "outputId": "aa3375bc-5f21-42b8-bd16-fa5ee5bd93bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Convolutional-KANs'...\n",
            "remote: Enumerating objects: 1539, done.\u001b[K\n",
            "remote: Counting objects: 100% (385/385), done.\u001b[K\n",
            "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
            "remote: Total 1539 (delta 253), reused 351 (delta 224), pack-reused 1154\u001b[K\n",
            "Receiving objects: 100% (1539/1539), 33.38 MiB | 12.09 MiB/s, done.\n",
            "Resolving deltas: 100% (727/727), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#this directory contains several files and directories, such as architectures_28x28, images, kan_convolutional, requirements.txt, etc.\n",
        "%cd Convolutional-KANs\n",
        "#installing the necessary packages\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-DiePk6u1rC",
        "outputId": "d31ec2f3-ca9b-444d-be95-1e2195387531"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Convolutional-KANs\n",
            "Collecting matplotlib==3.6.2 (from -r requirements.txt (line 1))\n",
            "  Downloading matplotlib-3.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.26.4)\n",
            "Collecting pandas==2.2.2 (from -r requirements.txt (line 3))\n",
            "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting scikit-learn==1.4.2 (from -r requirements.txt (line 4))\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting tqdm==4.66.4 (from -r requirements.txt (line 5))\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m863.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.3.0+cu118 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.3.0+cu118\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading necessary libraries\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "from architectures_28x28.CKAN_BN import CKAN_BN\n",
        "from architectures_28x28.SimpleModels import *\n",
        "from architectures_28x28.ConvNet import ConvNet\n",
        "from architectures_28x28.KANConvs_MLP import KANC_MLP\n",
        "from architectures_28x28.KKAN import KKAN_Convolutional_Network\n",
        "from architectures_28x28.conv_and_kan import NormalConvsKAN\n",
        "from kan_convolutional.KANConv import KAN_Convolutional_Layer"
      ],
      "metadata": {
        "id": "k9I0ASYjvYxv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Accessing the dataset"
      ],
      "metadata": {
        "id": "UDVlb_0V9gAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#defining transformations for the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #normalizing to [-1, 1]\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "#loading the MNIST dataset\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "#creating data loaders for training and testing\n",
        "#dataLoader (refer: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
      ],
      "metadata": {
        "id": "6TVKuuv2n5hV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fbaec99-b842-409c-e6dc-be6cf187845b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 32134474.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1031258.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 8932536.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3320064.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model training"
      ],
      "metadata": {
        "id": "R8FzyNzQ9Ovb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#implementation of KAN Convolution & 2 Layer MLP\n",
        "\n",
        "class KANC_MLP(nn.Module):\n",
        "    def __init__(self, device: str = 'cpu'):\n",
        "        super(KANC_MLP, self).__init__()\n",
        "\n",
        "        #setting first convolutional layer using KAN_Convolutional_Layer\n",
        "        self.conv1 = KAN_Convolutional_Layer(\n",
        "            n_convs=5,\n",
        "            kernel_size=(3, 3),\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        #setting second convolutional layer\n",
        "        self.conv2 = KAN_Convolutional_Layer(\n",
        "            n_convs=5,\n",
        "            kernel_size=(3, 3),\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        #setting max pooling layer\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "        #setting flatten layer to convert 2D feature maps to 1D vector\n",
        "        self.flat = nn.Flatten()\n",
        "\n",
        "        #setting fully connected layers\n",
        "        self.linear1 = nn.Linear(625, 256)\n",
        "        self.linear2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.flat(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "DRY0SPAKSeWx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking if GPU is available and use it if possible\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#initializing the model and move it to the appropriate device\n",
        "model_kanc = KANC_MLP(device=device).to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_kanc.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "SXDG2yVNSjUb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model for one epoch\n",
        "\n",
        "    Arguments:\n",
        "        1. model: the neural network model\n",
        "        2. device: cuda or cpu\n",
        "        3. train_loader: DataLoader for training data\n",
        "        4. optimizer: the optimizer to use (e.g. SGD)\n",
        "        5. epoch: the current epoch\n",
        "        6. criterion: the loss function (e.g. CrossEntropy)\n",
        "\n",
        "    Returns:\n",
        "        avg_loss: the average loss over the training set"
      ],
      "metadata": {
        "id": "2FxMZJt196IM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#setting epochs for training\n",
        "epochs = 1\n",
        "\n",
        "#training the model\n",
        "for epoch in range(epochs):\n",
        "    model_kanc.train()\n",
        "    total_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        #moving labels and images to the device (GPU or CPU)\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        #zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        #forward pass\n",
        "        outputs = model_kanc(images)\n",
        "        #calculating the loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        #backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #accumulating the loss for reporting\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    #printing the average loss for the epoch\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "412oWD9oSmZ1",
        "outputId": "f08ee312-9d7b-4e5f-daa6-aa96084df18e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1], Loss: 0.1505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model evaluation"
      ],
      "metadata": {
        "id": "oGtn6NmFLi7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluating the model\n",
        "model_kanc.eval()\n",
        "test_loss = 0\n",
        "correct = 0\n",
        "all_targets = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for images, labels in test_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    #getting the predicted classes for this batch\n",
        "    output = model_kanc(images)\n",
        "    #calculating the loss for this batch\n",
        "    test_loss += criterion(output, labels).item()\n",
        "    #calculating the accuracy for this batch\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    correct += (labels == predicted).sum().item()\n",
        "    #collecting all targets and predictions for metric calculations\n",
        "    all_targets.extend(labels.view_as(predicted).cpu().numpy())\n",
        "    all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "#normalizing test loss\n",
        "test_loss /= len(test_loader.dataset)\n",
        "#calculating accuracy\n",
        "accuracy = correct / len(test_loader.dataset)\n",
        "#calculating overall metrics\n",
        "precision = precision_score(all_targets, all_predictions, average='macro')\n",
        "recall = recall_score(all_targets, all_predictions, average='macro')\n",
        "f1 = f1_score(all_targets, all_predictions, average='macro')\n",
        "\n",
        "print('\\nTest set:\\n Accuracy: {:.2f}%, \\n Precision: {:.2f}, \\n Recall: {:.2f}, \\n F1 Score: {:.2f}\\n'.format(accuracy, precision, recall, f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3ON1g796HrG",
        "outputId": "216e381d-d492-4e5f-d96a-048baced6bc5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set:\n",
            " Accuracy: 0.98%, \n",
            " Precision: 0.98, \n",
            " Recall: 0.98, \n",
            " F1 Score: 0.98\n",
            "\n"
          ]
        }
      ]
    }
  ]
}